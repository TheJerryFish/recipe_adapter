program(1.0)
[buildInfo = dict<tensor<string, []>, tensor<string, []>>({{"coremlc-component-MIL", "3402.3.2"}, {"coremlc-version", "3402.4.1"}, {"coremltools-component-torch", "2.5.0"}, {"coremltools-source-dialect", "TorchScript"}, {"coremltools-version", "7.1"}})]
{
    func main<ios16>(tensor<int32, [1, 32]> attention_mask, tensor<int32, [1, 32]> input_ids) {
            tensor<int32, []> inputs_embeds_axis_0 = const()[name = tensor<string, []>("inputs_embeds_axis_0"), val = tensor<int32, []>(0)];
            tensor<int32, []> inputs_embeds_batch_dims_0 = const()[name = tensor<string, []>("inputs_embeds_batch_dims_0"), val = tensor<int32, []>(0)];
            tensor<fp16, [30522, 128]> base_bert_embeddings_word_embeddings_weight_to_fp16 = const()[name = tensor<string, []>("base_bert_embeddings_word_embeddings_weight_to_fp16"), val = tensor<fp16, [30522, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(64)))];
            tensor<fp16, [1, 32, 128]> inputs_embeds_cast_fp16 = gather(axis = inputs_embeds_axis_0, batch_dims = inputs_embeds_batch_dims_0, indices = input_ids, x = base_bert_embeddings_word_embeddings_weight_to_fp16)[name = tensor<string, []>("inputs_embeds_cast_fp16")];
            tensor<fp16, [1, 32, 128]> token_type_embeddings_1_to_fp16 = const()[name = tensor<string, []>("token_type_embeddings_1_to_fp16"), val = tensor<fp16, [1, 32, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7813760)))];
            tensor<fp16, [1, 32, 128]> embeddings_1_cast_fp16 = add(x = inputs_embeds_cast_fp16, y = token_type_embeddings_1_to_fp16)[name = tensor<string, []>("embeddings_1_cast_fp16")];
            tensor<fp16, [1, 32, 128]> position_embeddings_1_to_fp16 = const()[name = tensor<string, []>("position_embeddings_1_to_fp16"), val = tensor<fp16, [1, 32, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7822016)))];
            tensor<fp16, [1, 32, 128]> input_5_cast_fp16 = add(x = embeddings_1_cast_fp16, y = position_embeddings_1_to_fp16)[name = tensor<string, []>("input_5_cast_fp16")];
            tensor<int32, [1]> input_7_axes_0 = const()[name = tensor<string, []>("input_7_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [128]> base_bert_embeddings_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("base_bert_embeddings_LayerNorm_weight_to_fp16"), val = tensor<fp16, [128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7830272)))];
            tensor<fp16, [128]> base_bert_embeddings_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("base_bert_embeddings_LayerNorm_bias_to_fp16"), val = tensor<fp16, [128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7830592)))];
            tensor<fp16, []> var_10_to_fp16 = const()[name = tensor<string, []>("op_10_to_fp16"), val = tensor<fp16, []>(0x1p-24)];
            tensor<fp16, [1, 32, 128]> input_7_cast_fp16 = layer_norm(axes = input_7_axes_0, beta = base_bert_embeddings_LayerNorm_bias_to_fp16, epsilon = var_10_to_fp16, gamma = base_bert_embeddings_LayerNorm_weight_to_fp16, x = input_5_cast_fp16)[name = tensor<string, []>("input_7_cast_fp16")];
            tensor<int32, [1]> var_64_axes_0 = const()[name = tensor<string, []>("op_64_axes_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [1, 1, 32]> var_64 = expand_dims(axes = var_64_axes_0, x = attention_mask)[name = tensor<string, []>("op_64")];
            tensor<int32, [1]> var_65_axes_0 = const()[name = tensor<string, []>("op_65_axes_0"), val = tensor<int32, [1]>([2])];
            tensor<int32, [1, 1, 1, 32]> var_65 = expand_dims(axes = var_65_axes_0, x = var_64)[name = tensor<string, []>("op_65")];
            tensor<int32, [4]> var_68_reps_0 = const()[name = tensor<string, []>("op_68_reps_0"), val = tensor<int32, [4]>([1, 1, 32, 1])];
            tensor<int32, [1, 1, 32, 32]> var_68 = tile(reps = var_68_reps_0, x = var_65)[name = tensor<string, []>("op_68")];
            tensor<fp16, []> var_17_to_fp16 = const()[name = tensor<string, []>("op_17_to_fp16"), val = tensor<fp16, []>(0x1p+0)];
            tensor<string, []> var_69_to_fp16_dtype_0 = const()[name = tensor<string, []>("op_69_to_fp16_dtype_0"), val = tensor<string, []>("fp16")];
            tensor<fp16, [1, 1, 32, 32]> cast_13 = cast(dtype = var_69_to_fp16_dtype_0, x = var_68)[name = tensor<string, []>("cast_13")];
            tensor<fp16, [1, 1, 32, 32]> inverted_mask_cast_fp16 = sub(x = var_17_to_fp16, y = cast_13)[name = tensor<string, []>("inverted_mask_cast_fp16")];
            tensor<string, []> var_71_dtype_0 = const()[name = tensor<string, []>("op_71_dtype_0"), val = tensor<string, []>("bool")];
            tensor<fp16, []> var_19_to_fp16 = const()[name = tensor<string, []>("op_19_to_fp16"), val = tensor<fp16, []>(-inf)];
            tensor<bool, [1, 1, 32, 32]> cast_12 = cast(dtype = var_71_dtype_0, x = inverted_mask_cast_fp16)[name = tensor<string, []>("cast_12")];
            tensor<fp16, [1, 1, 32, 32]> attention_mask_cast_fp16 = select(a = var_19_to_fp16, b = inverted_mask_cast_fp16, cond = cast_12)[name = tensor<string, []>("attention_mask_cast_fp16")];
            tensor<fp16, [128, 128]> base_bert_encoder_layer_0_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("base_bert_encoder_layer_0_attention_self_query_weight_to_fp16"), val = tensor<fp16, [128, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7830912)))];
            tensor<fp16, [128]> base_bert_encoder_layer_0_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("base_bert_encoder_layer_0_attention_self_query_bias_to_fp16"), val = tensor<fp16, [128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7863744)))];
            tensor<fp16, [1, 32, 128]> linear_0_cast_fp16 = linear(bias = base_bert_encoder_layer_0_attention_self_query_bias_to_fp16, weight = base_bert_encoder_layer_0_attention_self_query_weight_to_fp16, x = input_7_cast_fp16)[name = tensor<string, []>("linear_0_cast_fp16")];
            tensor<int32, [4]> var_92 = const()[name = tensor<string, []>("op_92"), val = tensor<int32, [4]>([1, 32, 2, 64])];
            tensor<fp16, [1, 32, 2, 64]> x_3_cast_fp16 = reshape(shape = var_92, x = linear_0_cast_fp16)[name = tensor<string, []>("x_3_cast_fp16")];
            tensor<fp16, [128, 128]> base_bert_encoder_layer_0_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("base_bert_encoder_layer_0_attention_self_key_weight_to_fp16"), val = tensor<fp16, [128, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7864064)))];
            tensor<fp16, [128]> base_bert_encoder_layer_0_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("base_bert_encoder_layer_0_attention_self_key_bias_to_fp16"), val = tensor<fp16, [128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7896896)))];
            tensor<fp16, [1, 32, 128]> linear_1_cast_fp16 = linear(bias = base_bert_encoder_layer_0_attention_self_key_bias_to_fp16, weight = base_bert_encoder_layer_0_attention_self_key_weight_to_fp16, x = input_7_cast_fp16)[name = tensor<string, []>("linear_1_cast_fp16")];
            tensor<int32, [4]> var_101 = const()[name = tensor<string, []>("op_101"), val = tensor<int32, [4]>([1, 32, 2, 64])];
            tensor<fp16, [1, 32, 2, 64]> x_7_cast_fp16 = reshape(shape = var_101, x = linear_1_cast_fp16)[name = tensor<string, []>("x_7_cast_fp16")];
            tensor<fp16, [128, 128]> base_bert_encoder_layer_0_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("base_bert_encoder_layer_0_attention_self_value_weight_to_fp16"), val = tensor<fp16, [128, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7897216)))];
            tensor<fp16, [128]> base_bert_encoder_layer_0_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("base_bert_encoder_layer_0_attention_self_value_bias_to_fp16"), val = tensor<fp16, [128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7930048)))];
            tensor<fp16, [1, 32, 128]> linear_2_cast_fp16 = linear(bias = base_bert_encoder_layer_0_attention_self_value_bias_to_fp16, weight = base_bert_encoder_layer_0_attention_self_value_weight_to_fp16, x = input_7_cast_fp16)[name = tensor<string, []>("linear_2_cast_fp16")];
            tensor<int32, [4]> var_110 = const()[name = tensor<string, []>("op_110"), val = tensor<int32, [4]>([1, 32, 2, 64])];
            tensor<fp16, [1, 32, 2, 64]> x_11_cast_fp16 = reshape(shape = var_110, x = linear_2_cast_fp16)[name = tensor<string, []>("x_11_cast_fp16")];
            tensor<int32, [4]> var_112 = const()[name = tensor<string, []>("op_112"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, []> mul_0_y_0_to_fp16 = const()[name = tensor<string, []>("mul_0_y_0_to_fp16"), val = tensor<fp16, []>(0x1p-3)];
            tensor<fp16, [1, 32, 2, 64]> mul_0_cast_fp16 = mul(x = x_3_cast_fp16, y = mul_0_y_0_to_fp16)[name = tensor<string, []>("mul_0_cast_fp16")];
            tensor<bool, []> matmul_0_transpose_y_0 = const()[name = tensor<string, []>("matmul_0_transpose_y_0"), val = tensor<bool, []>(true)];
            tensor<bool, []> matmul_0_transpose_x_0 = const()[name = tensor<string, []>("matmul_0_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_8_perm_0 = const()[name = tensor<string, []>("transpose_8_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> transpose_9_perm_0 = const()[name = tensor<string, []>("transpose_9_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, [1, 2, 32, 64]> transpose_17 = transpose(perm = transpose_9_perm_0, x = x_7_cast_fp16)[name = tensor<string, []>("transpose_17")];
            tensor<fp16, [1, 2, 32, 64]> transpose_18 = transpose(perm = transpose_8_perm_0, x = mul_0_cast_fp16)[name = tensor<string, []>("transpose_18")];
            tensor<fp16, [1, 2, 32, 32]> matmul_0_cast_fp16 = matmul(transpose_x = matmul_0_transpose_x_0, transpose_y = matmul_0_transpose_y_0, x = transpose_18, y = transpose_17)[name = tensor<string, []>("matmul_0_cast_fp16")];
            tensor<fp16, [1, 2, 32, 32]> add_0_cast_fp16 = add(x = matmul_0_cast_fp16, y = attention_mask_cast_fp16)[name = tensor<string, []>("add_0_cast_fp16")];
            tensor<int32, []> softmax_0_axis_0 = const()[name = tensor<string, []>("softmax_0_axis_0"), val = tensor<int32, []>(-1)];
            tensor<fp16, [1, 2, 32, 32]> softmax_0_cast_fp16 = softmax(axis = softmax_0_axis_0, x = add_0_cast_fp16)[name = tensor<string, []>("softmax_0_cast_fp16")];
            tensor<bool, []> attn_output_1_transpose_x_0 = const()[name = tensor<string, []>("attn_output_1_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attn_output_1_transpose_y_0 = const()[name = tensor<string, []>("attn_output_1_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 2, 32, 64]> transpose_19 = transpose(perm = var_112, x = x_11_cast_fp16)[name = tensor<string, []>("transpose_19")];
            tensor<fp16, [1, 2, 32, 64]> attn_output_1_cast_fp16 = matmul(transpose_x = attn_output_1_transpose_x_0, transpose_y = attn_output_1_transpose_y_0, x = softmax_0_cast_fp16, y = transpose_19)[name = tensor<string, []>("attn_output_1_cast_fp16")];
            tensor<int32, [4]> attn_output_3_perm_0 = const()[name = tensor<string, []>("attn_output_3_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_116 = const()[name = tensor<string, []>("op_116"), val = tensor<int32, [3]>([1, 32, 128])];
            tensor<fp16, [1, 32, 2, 64]> transpose_16 = transpose(perm = attn_output_3_perm_0, x = attn_output_1_cast_fp16)[name = tensor<string, []>("transpose_16")];
            tensor<fp16, [1, 32, 128]> input_9_cast_fp16 = reshape(shape = var_116, x = transpose_16)[name = tensor<string, []>("input_9_cast_fp16")];
            tensor<fp16, [128, 128]> base_bert_encoder_layer_0_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("base_bert_encoder_layer_0_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [128, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7930368)))];
            tensor<fp16, [128]> base_bert_encoder_layer_0_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("base_bert_encoder_layer_0_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7963200)))];
            tensor<fp16, [1, 32, 128]> linear_3_cast_fp16 = linear(bias = base_bert_encoder_layer_0_attention_output_dense_bias_to_fp16, weight = base_bert_encoder_layer_0_attention_output_dense_weight_to_fp16, x = input_9_cast_fp16)[name = tensor<string, []>("linear_3_cast_fp16")];
            tensor<fp16, [1, 32, 128]> input_13_cast_fp16 = add(x = linear_3_cast_fp16, y = input_7_cast_fp16)[name = tensor<string, []>("input_13_cast_fp16")];
            tensor<int32, [1]> input_15_axes_0 = const()[name = tensor<string, []>("input_15_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [128]> base_bert_encoder_layer_0_attention_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("base_bert_encoder_layer_0_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7963520)))];
            tensor<fp16, [128]> base_bert_encoder_layer_0_attention_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("base_bert_encoder_layer_0_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7963840)))];
            tensor<fp16, [1, 32, 128]> input_15_cast_fp16 = layer_norm(axes = input_15_axes_0, beta = base_bert_encoder_layer_0_attention_output_LayerNorm_bias_to_fp16, epsilon = var_10_to_fp16, gamma = base_bert_encoder_layer_0_attention_output_LayerNorm_weight_to_fp16, x = input_13_cast_fp16)[name = tensor<string, []>("input_15_cast_fp16")];
            tensor<fp16, [512, 128]> base_bert_encoder_layer_0_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("base_bert_encoder_layer_0_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [512, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(7964160)))];
            tensor<fp16, [512]> base_bert_encoder_layer_0_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("base_bert_encoder_layer_0_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [512]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8095296)))];
            tensor<fp16, [1, 32, 512]> linear_4_cast_fp16 = linear(bias = base_bert_encoder_layer_0_intermediate_dense_bias_to_fp16, weight = base_bert_encoder_layer_0_intermediate_dense_weight_to_fp16, x = input_15_cast_fp16)[name = tensor<string, []>("linear_4_cast_fp16")];
            tensor<string, []> input_19_mode_0 = const()[name = tensor<string, []>("input_19_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, 32, 512]> input_19_cast_fp16 = gelu(mode = input_19_mode_0, x = linear_4_cast_fp16)[name = tensor<string, []>("input_19_cast_fp16")];
            tensor<fp16, [128, 512]> base_bert_encoder_layer_0_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("base_bert_encoder_layer_0_output_dense_weight_to_fp16"), val = tensor<fp16, [128, 512]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8096384)))];
            tensor<fp16, [128]> base_bert_encoder_layer_0_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("base_bert_encoder_layer_0_output_dense_bias_to_fp16"), val = tensor<fp16, [128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8227520)))];
            tensor<fp16, [1, 32, 128]> linear_5_cast_fp16 = linear(bias = base_bert_encoder_layer_0_output_dense_bias_to_fp16, weight = base_bert_encoder_layer_0_output_dense_weight_to_fp16, x = input_19_cast_fp16)[name = tensor<string, []>("linear_5_cast_fp16")];
            tensor<fp16, [1, 32, 128]> input_23_cast_fp16 = add(x = linear_5_cast_fp16, y = input_15_cast_fp16)[name = tensor<string, []>("input_23_cast_fp16")];
            tensor<int32, [1]> hidden_states_7_axes_0 = const()[name = tensor<string, []>("hidden_states_7_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [128]> base_bert_encoder_layer_0_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("base_bert_encoder_layer_0_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8227840)))];
            tensor<fp16, [128]> base_bert_encoder_layer_0_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("base_bert_encoder_layer_0_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8228160)))];
            tensor<fp16, [1, 32, 128]> hidden_states_7_cast_fp16 = layer_norm(axes = hidden_states_7_axes_0, beta = base_bert_encoder_layer_0_output_LayerNorm_bias_to_fp16, epsilon = var_10_to_fp16, gamma = base_bert_encoder_layer_0_output_LayerNorm_weight_to_fp16, x = input_23_cast_fp16)[name = tensor<string, []>("hidden_states_7_cast_fp16")];
            tensor<fp16, [128, 128]> base_bert_encoder_layer_1_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("base_bert_encoder_layer_1_attention_self_query_weight_to_fp16"), val = tensor<fp16, [128, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8228480)))];
            tensor<fp16, [128]> base_bert_encoder_layer_1_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("base_bert_encoder_layer_1_attention_self_query_bias_to_fp16"), val = tensor<fp16, [128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8261312)))];
            tensor<fp16, [1, 32, 128]> linear_6_cast_fp16 = linear(bias = base_bert_encoder_layer_1_attention_self_query_bias_to_fp16, weight = base_bert_encoder_layer_1_attention_self_query_weight_to_fp16, x = hidden_states_7_cast_fp16)[name = tensor<string, []>("linear_6_cast_fp16")];
            tensor<int32, [4]> var_160 = const()[name = tensor<string, []>("op_160"), val = tensor<int32, [4]>([1, 32, 2, 64])];
            tensor<fp16, [1, 32, 2, 64]> x_15_cast_fp16 = reshape(shape = var_160, x = linear_6_cast_fp16)[name = tensor<string, []>("x_15_cast_fp16")];
            tensor<fp16, [128, 128]> base_bert_encoder_layer_1_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("base_bert_encoder_layer_1_attention_self_key_weight_to_fp16"), val = tensor<fp16, [128, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8261632)))];
            tensor<fp16, [128]> base_bert_encoder_layer_1_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("base_bert_encoder_layer_1_attention_self_key_bias_to_fp16"), val = tensor<fp16, [128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8294464)))];
            tensor<fp16, [1, 32, 128]> linear_7_cast_fp16 = linear(bias = base_bert_encoder_layer_1_attention_self_key_bias_to_fp16, weight = base_bert_encoder_layer_1_attention_self_key_weight_to_fp16, x = hidden_states_7_cast_fp16)[name = tensor<string, []>("linear_7_cast_fp16")];
            tensor<int32, [4]> var_169 = const()[name = tensor<string, []>("op_169"), val = tensor<int32, [4]>([1, 32, 2, 64])];
            tensor<fp16, [1, 32, 2, 64]> x_19_cast_fp16 = reshape(shape = var_169, x = linear_7_cast_fp16)[name = tensor<string, []>("x_19_cast_fp16")];
            tensor<fp16, [128, 128]> base_bert_encoder_layer_1_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("base_bert_encoder_layer_1_attention_self_value_weight_to_fp16"), val = tensor<fp16, [128, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8294784)))];
            tensor<fp16, [128]> base_bert_encoder_layer_1_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("base_bert_encoder_layer_1_attention_self_value_bias_to_fp16"), val = tensor<fp16, [128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8327616)))];
            tensor<fp16, [1, 32, 128]> linear_8_cast_fp16 = linear(bias = base_bert_encoder_layer_1_attention_self_value_bias_to_fp16, weight = base_bert_encoder_layer_1_attention_self_value_weight_to_fp16, x = hidden_states_7_cast_fp16)[name = tensor<string, []>("linear_8_cast_fp16")];
            tensor<int32, [4]> var_178 = const()[name = tensor<string, []>("op_178"), val = tensor<int32, [4]>([1, 32, 2, 64])];
            tensor<fp16, [1, 32, 2, 64]> x_cast_fp16 = reshape(shape = var_178, x = linear_8_cast_fp16)[name = tensor<string, []>("x_cast_fp16")];
            tensor<int32, [4]> var_180 = const()[name = tensor<string, []>("op_180"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, []> mul_1_y_0_to_fp16 = const()[name = tensor<string, []>("mul_1_y_0_to_fp16"), val = tensor<fp16, []>(0x1p-3)];
            tensor<fp16, [1, 32, 2, 64]> mul_1_cast_fp16 = mul(x = x_15_cast_fp16, y = mul_1_y_0_to_fp16)[name = tensor<string, []>("mul_1_cast_fp16")];
            tensor<bool, []> matmul_1_transpose_y_0 = const()[name = tensor<string, []>("matmul_1_transpose_y_0"), val = tensor<bool, []>(true)];
            tensor<bool, []> matmul_1_transpose_x_0 = const()[name = tensor<string, []>("matmul_1_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_10_perm_0 = const()[name = tensor<string, []>("transpose_10_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> transpose_11_perm_0 = const()[name = tensor<string, []>("transpose_11_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, [1, 2, 32, 64]> transpose_13 = transpose(perm = transpose_11_perm_0, x = x_19_cast_fp16)[name = tensor<string, []>("transpose_13")];
            tensor<fp16, [1, 2, 32, 64]> transpose_14 = transpose(perm = transpose_10_perm_0, x = mul_1_cast_fp16)[name = tensor<string, []>("transpose_14")];
            tensor<fp16, [1, 2, 32, 32]> matmul_1_cast_fp16 = matmul(transpose_x = matmul_1_transpose_x_0, transpose_y = matmul_1_transpose_y_0, x = transpose_14, y = transpose_13)[name = tensor<string, []>("matmul_1_cast_fp16")];
            tensor<fp16, [1, 2, 32, 32]> add_1_cast_fp16 = add(x = matmul_1_cast_fp16, y = attention_mask_cast_fp16)[name = tensor<string, []>("add_1_cast_fp16")];
            tensor<int32, []> softmax_1_axis_0 = const()[name = tensor<string, []>("softmax_1_axis_0"), val = tensor<int32, []>(-1)];
            tensor<fp16, [1, 2, 32, 32]> softmax_1_cast_fp16 = softmax(axis = softmax_1_axis_0, x = add_1_cast_fp16)[name = tensor<string, []>("softmax_1_cast_fp16")];
            tensor<bool, []> attn_output_5_transpose_x_0 = const()[name = tensor<string, []>("attn_output_5_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attn_output_5_transpose_y_0 = const()[name = tensor<string, []>("attn_output_5_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 2, 32, 64]> transpose_15 = transpose(perm = var_180, x = x_cast_fp16)[name = tensor<string, []>("transpose_15")];
            tensor<fp16, [1, 2, 32, 64]> attn_output_5_cast_fp16 = matmul(transpose_x = attn_output_5_transpose_x_0, transpose_y = attn_output_5_transpose_y_0, x = softmax_1_cast_fp16, y = transpose_15)[name = tensor<string, []>("attn_output_5_cast_fp16")];
            tensor<int32, [4]> attn_output_perm_0 = const()[name = tensor<string, []>("attn_output_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_184 = const()[name = tensor<string, []>("op_184"), val = tensor<int32, [3]>([1, 32, 128])];
            tensor<fp16, [1, 32, 2, 64]> transpose_12 = transpose(perm = attn_output_perm_0, x = attn_output_5_cast_fp16)[name = tensor<string, []>("transpose_12")];
            tensor<fp16, [1, 32, 128]> input_25_cast_fp16 = reshape(shape = var_184, x = transpose_12)[name = tensor<string, []>("input_25_cast_fp16")];
            tensor<fp16, [128, 128]> base_bert_encoder_layer_1_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("base_bert_encoder_layer_1_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [128, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8327936)))];
            tensor<fp16, [128]> base_bert_encoder_layer_1_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("base_bert_encoder_layer_1_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8360768)))];
            tensor<fp16, [1, 32, 128]> linear_9_cast_fp16 = linear(bias = base_bert_encoder_layer_1_attention_output_dense_bias_to_fp16, weight = base_bert_encoder_layer_1_attention_output_dense_weight_to_fp16, x = input_25_cast_fp16)[name = tensor<string, []>("linear_9_cast_fp16")];
            tensor<fp16, [1, 32, 128]> input_29_cast_fp16 = add(x = linear_9_cast_fp16, y = hidden_states_7_cast_fp16)[name = tensor<string, []>("input_29_cast_fp16")];
            tensor<int32, [1]> input_31_axes_0 = const()[name = tensor<string, []>("input_31_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [128]> base_bert_encoder_layer_1_attention_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("base_bert_encoder_layer_1_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8361088)))];
            tensor<fp16, [128]> base_bert_encoder_layer_1_attention_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("base_bert_encoder_layer_1_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8361408)))];
            tensor<fp16, [1, 32, 128]> input_31_cast_fp16 = layer_norm(axes = input_31_axes_0, beta = base_bert_encoder_layer_1_attention_output_LayerNorm_bias_to_fp16, epsilon = var_10_to_fp16, gamma = base_bert_encoder_layer_1_attention_output_LayerNorm_weight_to_fp16, x = input_29_cast_fp16)[name = tensor<string, []>("input_31_cast_fp16")];
            tensor<fp16, [512, 128]> base_bert_encoder_layer_1_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("base_bert_encoder_layer_1_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [512, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8361728)))];
            tensor<fp16, [512]> base_bert_encoder_layer_1_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("base_bert_encoder_layer_1_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [512]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8492864)))];
            tensor<fp16, [1, 32, 512]> linear_10_cast_fp16 = linear(bias = base_bert_encoder_layer_1_intermediate_dense_bias_to_fp16, weight = base_bert_encoder_layer_1_intermediate_dense_weight_to_fp16, x = input_31_cast_fp16)[name = tensor<string, []>("linear_10_cast_fp16")];
            tensor<string, []> input_35_mode_0 = const()[name = tensor<string, []>("input_35_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, 32, 512]> input_35_cast_fp16 = gelu(mode = input_35_mode_0, x = linear_10_cast_fp16)[name = tensor<string, []>("input_35_cast_fp16")];
            tensor<fp16, [128, 512]> base_bert_encoder_layer_1_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("base_bert_encoder_layer_1_output_dense_weight_to_fp16"), val = tensor<fp16, [128, 512]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8493952)))];
            tensor<fp16, [128]> base_bert_encoder_layer_1_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("base_bert_encoder_layer_1_output_dense_bias_to_fp16"), val = tensor<fp16, [128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8625088)))];
            tensor<fp16, [1, 32, 128]> linear_11_cast_fp16 = linear(bias = base_bert_encoder_layer_1_output_dense_bias_to_fp16, weight = base_bert_encoder_layer_1_output_dense_weight_to_fp16, x = input_35_cast_fp16)[name = tensor<string, []>("linear_11_cast_fp16")];
            tensor<fp16, [1, 32, 128]> input_39_cast_fp16 = add(x = linear_11_cast_fp16, y = input_31_cast_fp16)[name = tensor<string, []>("input_39_cast_fp16")];
            tensor<int32, [1]> hidden_states_axes_0 = const()[name = tensor<string, []>("hidden_states_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [128]> base_bert_encoder_layer_1_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("base_bert_encoder_layer_1_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8625408)))];
            tensor<fp16, [128]> base_bert_encoder_layer_1_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("base_bert_encoder_layer_1_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8625728)))];
            tensor<fp16, [1, 32, 128]> hidden_states_cast_fp16 = layer_norm(axes = hidden_states_axes_0, beta = base_bert_encoder_layer_1_output_LayerNorm_bias_to_fp16, epsilon = var_10_to_fp16, gamma = base_bert_encoder_layer_1_output_LayerNorm_weight_to_fp16, x = input_39_cast_fp16)[name = tensor<string, []>("hidden_states_cast_fp16")];
            tensor<int32, [3]> input_41_begin_0 = const()[name = tensor<string, []>("input_41_begin_0"), val = tensor<int32, [3]>([0, 0, 0])];
            tensor<int32, [3]> input_41_end_0 = const()[name = tensor<string, []>("input_41_end_0"), val = tensor<int32, [3]>([1, 1, 128])];
            tensor<bool, [3]> input_41_end_mask_0 = const()[name = tensor<string, []>("input_41_end_mask_0"), val = tensor<bool, [3]>([true, false, true])];
            tensor<bool, [3]> input_41_squeeze_mask_0 = const()[name = tensor<string, []>("input_41_squeeze_mask_0"), val = tensor<bool, [3]>([false, true, false])];
            tensor<fp16, [1, 128]> input_41_cast_fp16 = slice_by_index(begin = input_41_begin_0, end = input_41_end_0, end_mask = input_41_end_mask_0, squeeze_mask = input_41_squeeze_mask_0, x = hidden_states_cast_fp16)[name = tensor<string, []>("input_41_cast_fp16")];
            tensor<fp16, [128, 128]> base_bert_pooler_dense_weight_to_fp16 = const()[name = tensor<string, []>("base_bert_pooler_dense_weight_to_fp16"), val = tensor<fp16, [128, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8626048)))];
            tensor<fp16, [128]> base_bert_pooler_dense_bias_to_fp16 = const()[name = tensor<string, []>("base_bert_pooler_dense_bias_to_fp16"), val = tensor<fp16, [128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8658880)))];
            tensor<fp16, [1, 128]> linear_12_cast_fp16 = linear(bias = base_bert_pooler_dense_bias_to_fp16, weight = base_bert_pooler_dense_weight_to_fp16, x = input_41_cast_fp16)[name = tensor<string, []>("linear_12_cast_fp16")];
            tensor<fp16, [1, 128]> input_45_cast_fp16 = tanh(x = linear_12_cast_fp16)[name = tensor<string, []>("input_45_cast_fp16")];
            tensor<fp16, [2, 128]> base_classifier_weight_to_fp16 = const()[name = tensor<string, []>("base_classifier_weight_to_fp16"), val = tensor<fp16, [2, 128]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(8659200)))];
            tensor<fp16, [2]> base_classifier_bias_to_fp16 = const()[name = tensor<string, []>("base_classifier_bias_to_fp16"), val = tensor<fp16, [2]>([0x0p+0, 0x0p+0])];
            tensor<fp16, [1, 2]> linear_13_cast_fp16 = linear(bias = base_classifier_bias_to_fp16, weight = base_classifier_weight_to_fp16, x = input_45_cast_fp16)[name = tensor<string, []>("linear_13_cast_fp16")];
            tensor<string, []> linear_13_cast_fp16_to_fp32_dtype_0 = const()[name = tensor<string, []>("linear_13_cast_fp16_to_fp32_dtype_0"), val = tensor<string, []>("fp32")];
            tensor<fp32, [1, 2]> logits = cast(dtype = linear_13_cast_fp16_to_fp32_dtype_0, x = linear_13_cast_fp16)[name = tensor<string, []>("cast_11")];
        } -> (logits);
}